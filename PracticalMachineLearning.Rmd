---
title: "Final Project Report - Practical Machine Learning Course"
author: "Karen Linden"
date: "Saturday, September 26, 2015"
output:
  html_document:
    keep_md: yes
    toc: yes    
---



##Reproduceablity
In order to reproduce the same results, you need a certain set of packages, as well as setting a pseudo random seed equal to the one I used. 
The following Libraries were used for this project, which you should install - if not done yet - and load on your working environment.

```{r echo = TRUE, warning=FALSE }

setwd('~/R/PracticalMachineLearning')

library(lattice)
library(ggplot2)
library(RGtk2)
library(splines)
library(survival)
library(plyr)
library(parallel)
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(gbm)

```

Finally, load the same seed with the following line of code:

```{r}
set.seed(12345)

```

##Getting and loading the data

```{r}
trnUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
tstUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trnUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(tstUrl), na.strings=c("NA","#DIV/0!",""))

```
Partioning the training set into two

```{r}
inTrn <- createDataPartition(training$classe, p=0.6, list=FALSE)
Train<- training[inTrn, ]
Test <- training[-inTrn, ]
dim(Train); dim(Test)

```

##Cleaning the data

Cleaning NearZeroVariance Variables Run this code to view possible NZV Variables:

```{r}
DataNZV <- nearZeroVar(Train, saveMetrics=TRUE)
Train <- Train[,DataNZV$nzv==FALSE]

```

Remove the first column of the myTraining data set
```{r}
Train <- Train[c(-1)]

```

Clean variables with more than 60% NA
```{r}
train2 <- Train 
for(i in 1:length(Train)) {
    if( sum( is.na(Train[, i] ) ) /nrow(Train) >= .7) {
        for(j in 1:length(train2)) {
            if( length( grep(names(Train[i]), names(train2)[j]) ) == 1)  {
                train2 <- train2[ , -j]
            }   
        } 
    }
}

Train <- train2
rm(train2)

```
Now let us do transformations but for our myTesting and testing data sets.
```{r}
c1 <- colnames(Train)
c2 <- colnames(Train[, -58])  
Test<- Test[c1]        
testing <- testing[c2]           

dim(Test)

dim(testing)

```

In order to ensure proper functioning of Decision Trees and especially RandomForest Algorithm with the Test data set (data set provided), we need to coerce the data into the same type.

```{r}
for (i in 1:length(testing) ) {
    for(j in 1:length(Train)) {
        if( length( grep(names(Train[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(Train[i])
        }      
    }      
}

testing <- rbind(Train[2, -58] , testing) 
testing <- testing[-1,]

```

##Prediction with Decision Trees
```{r}
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=Train, method="class")
fancyRpartPlot(modFitA1)

predictionsA1 <- predict(modFitA1, Test, type = "class")
cmtree <- confusionMatrix(predictionsA1, Test$classe)
cmtree

```

##Prediction with Random Forests
```{r}
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=Train)
predictionB1 <- predict(modFitB1, Test, type = "class")
cmrf <- confusionMatrix(predictionB1, Test$classe)
cmrf

plot(modFitB1)

plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))

```


##Prediction with Generalized Boosted Regression
```{r}
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

gbmFit1 <- train(classe ~ ., data=Train, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)


gbmFinMod1 <- gbmFit1$finalModel

gbmPredTest <- predict(gbmFit1, newdata=Test)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, Test$classe)
gbmAccuracyTest

plot(gbmFit1, ylim=c(0.9, 1))

```
##Predicting Results on the Test Data
Random Forests gave an Accuracy in the Test dataset of 99.89%, which was more accurate that what I got from the Decision Trees or GBM. The expected out-of-sample error is 100-99.89 = 0.11%.
```{r}
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2

pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(predictionB2)

```




